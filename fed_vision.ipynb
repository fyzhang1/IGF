{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import label_to_onehot, cross_entropy_for_onehot\n",
    "from models.vision import weights_init, LeNet\n",
    "num_classes=10\n",
    "\n",
    "net = LeNet(num_classes).to(\"cuda\")\n",
    "compress_rate = 1.0\n",
    "torch.manual_seed(1234)\n",
    "net.apply(weights_init)\n",
    "criterion = cross_entropy_for_onehot\n",
    "\n",
    "\n",
    "def federated_train(global_model, client_loaders, criterion, num_rounds=10, num_local_epochs=1, lr=0.001):\n",
    "    \"\"\"联邦训练函数(FedAvg)\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    global_model.to(device)\n",
    "    \n",
    "    for round in range(num_rounds):\n",
    "        print(f\"Communication Round {round+1}/{num_rounds}\")\n",
    "        client_models = []\n",
    "        \n",
    "        # 训练所有客户端\n",
    "        for client_id, loader in enumerate(client_loaders):\n",
    "            # 克隆全局模型\n",
    "            local_model = LeNet(num_classes=10)\n",
    "            local_model.load_state_dict(global_model.state_dict())\n",
    "            local_model.to(device)\n",
    "            optimizer = optim.Adam(local_model.parameters(), lr=lr)\n",
    "            \n",
    "            # 本地训练\n",
    "            local_model.train()\n",
    "            for _ in range(num_local_epochs):\n",
    "                for images, labels in loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = local_model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # 保存客户端模型参数\n",
    "            client_models.append(local_model.state_dict())\n",
    "        \n",
    "        # 参数平均（FedAvg）\n",
    "        global_dict = global_model.state_dict()\n",
    "        for key in global_dict.keys():\n",
    "            global_dict[key] = torch.stack(\n",
    "                [client_models[i][key].float() for i in range(len(client_models))], 0\n",
    "            ).mean(0)\n",
    "        global_model.load_state_dict(global_dict)\n",
    "    \n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test固定样本和数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证通过：遗忘数据集样本保持固定\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# 设置全局随机种子保证可重复性\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# 数据加载配置\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dst_train = datasets.CIFAR10(\n",
    "    root=\"~/.torch\", \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 定义客户端数量和遗忘参数\n",
    "CLIENT_NUM = 4\n",
    "FORGOTTEN_CLIENT_IDX = 3  # 要遗忘的客户端索引\n",
    "FORGET_SIZE = 1000        # 固定遗忘样本数\n",
    "\n",
    "\n",
    "# 固定划分客户端数据（使用确定性的随机划分）\n",
    "client_datasets = torch.utils.data.random_split(\n",
    "    dst_train,\n",
    "    [len(dst_train)//CLIENT_NUM]*CLIENT_NUM,\n",
    "    generator=torch.Generator().manual_seed(SEED)  # 固定划分随机种子\n",
    ")\n",
    "\n",
    "# 获取目标客户端原始数据索引\n",
    "target_dataset = client_datasets[FORGOTTEN_CLIENT_IDX]\n",
    "original_indices = target_dataset.indices.copy()  # 原始索引列表\n",
    "\n",
    "# 确定性地选择前N个样本作为遗忘集（方法1：绝对位置）\n",
    "fixed_forgotten_indices = sorted(original_indices)[:FORGET_SIZE]  # 按原始顺序取前1000\n",
    "\n",
    "# 更新客户端数据集划分\n",
    "remaining_indices = list(set(original_indices) - set(fixed_forgotten_indices))\n",
    "client_datasets[FORGOTTEN_CLIENT_IDX] = Subset(dst_train, remaining_indices)\n",
    "\n",
    "# 创建遗忘数据集加载器\n",
    "forgotten_dataset = Subset(dst_train, fixed_forgotten_indices)\n",
    "forgotten_loader = DataLoader(\n",
    "    forgotten_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 创建客户端加载器（包含更新后的数据集）\n",
    "client_loaders = [\n",
    "    DataLoader(\n",
    "        ds, \n",
    "        batch_size=128, \n",
    "        shuffle=True,  # 训练时保持shuffle但随机种子固定\n",
    "        generator=torch.Generator().manual_seed(SEED))\n",
    "    for ds in client_datasets ]\n",
    "\n",
    "# 验证固定遗忘样本\n",
    "def verify_fixed_samples():\n",
    "    # 第一次运行获取样本特征\n",
    "    first_run_samples = []\n",
    "    for batch in forgotten_loader:\n",
    "        first_run_samples.append(batch[0].sum().item())\n",
    "    first_sum = sum(first_run_samples)\n",
    "    \n",
    "    # 第二次运行应该完全相同\n",
    "    second_run_samples = []\n",
    "    for batch in forgotten_loader:\n",
    "        second_run_samples.append(batch[0].sum().item())\n",
    "    \n",
    "    assert np.allclose(first_sum, sum(second_run_samples)), \"样本不固定!\"\n",
    "    print(\"验证通过：遗忘数据集样本保持固定\")\n",
    "\n",
    "verify_fixed_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "联邦训练完整模型...\n",
      "Communication Round 1/20\n",
      "Communication Round 2/20\n",
      "Communication Round 3/20\n",
      "Communication Round 4/20\n",
      "Communication Round 5/20\n",
      "Communication Round 6/20\n",
      "Communication Round 7/20\n",
      "Communication Round 8/20\n",
      "Communication Round 9/20\n",
      "Communication Round 10/20\n",
      "Communication Round 11/20\n",
      "Communication Round 12/20\n",
      "Communication Round 13/20\n",
      "Communication Round 14/20\n",
      "Communication Round 15/20\n",
      "Communication Round 16/20\n",
      "Communication Round 17/20\n",
      "Communication Round 18/20\n",
      "Communication Round 19/20\n",
      "Communication Round 20/20\n",
      "federated unlearning training...\n",
      "Communication Round 1/20\n",
      "Communication Round 2/20\n",
      "Communication Round 3/20\n",
      "Communication Round 4/20\n",
      "Communication Round 5/20\n",
      "Communication Round 6/20\n",
      "Communication Round 7/20\n",
      "Communication Round 8/20\n",
      "Communication Round 9/20\n",
      "Communication Round 10/20\n",
      "Communication Round 11/20\n",
      "Communication Round 12/20\n",
      "Communication Round 13/20\n",
      "Communication Round 14/20\n",
      "Communication Round 15/20\n",
      "Communication Round 16/20\n",
      "Communication Round 17/20\n",
      "Communication Round 18/20\n",
      "Communication Round 19/20\n",
      "Communication Round 20/20\n"
     ]
    }
   ],
   "source": [
    "client_batch_size = 128\n",
    "\n",
    "print(\"联邦训练完整模型...\")\n",
    "# 初始化全局模型\n",
    "full_net = LeNet(num_classes=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "global_round = 20\n",
    "\n",
    "full_net = federated_train(\n",
    "    full_net,\n",
    "    client_loaders,  # 包含调整后的客户端3数据\n",
    "    criterion,\n",
    "    num_rounds=global_round,\n",
    "    num_local_epochs=10,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# 未学习模型训练（从原始客户端加载器重建）\n",
    "# 需要重新加载原始客户端数据（排除遗忘样本）\n",
    "modified_client_loaders = [\n",
    "    DataLoader(\n",
    "        ds if idx != FORGOTTEN_CLIENT_IDX else Subset(ds.dataset, remaining_indices),\n",
    "        batch_size=client_batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    for idx, ds in enumerate(client_datasets)\n",
    "]\n",
    "\n",
    "unlearned_net = LeNet(num_classes=10)\n",
    "print(\"federated unlearning training...\")\n",
    "unlearned_net = federated_train(\n",
    "    unlearned_net,\n",
    "    modified_client_loaders,  # 使用排除遗忘样本的加载器\n",
    "    criterion,\n",
    "    num_rounds=global_round,\n",
    "    num_local_epochs=10,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(full_net.state_dict(), \"federated_full_sample_1000_round_20_partial.pth\")\n",
    "torch.save(unlearned_net.state_dict(), \"federated_unlearned_sample_1000_round_20_partial.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度上升"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 在 federated_unlearning_gradient_ascent 中直接使用\n",
    "def federated_unlearning_gradient_ascent(global_model, forgotten_loader, client_loaders, criterion, num_rounds=5, lr=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    global_model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(global_model.parameters(), lr=lr)\n",
    "    \n",
    "    print(\"Performing gradient ascent on forgotten data...\")\n",
    "    global_model.train()\n",
    "    for _ in tqdm(range(num_rounds)):\n",
    "        for images, labels in forgotten_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = global_model(images)\n",
    "            loss = criterion(outputs, labels)  # 直接使用类别索引\n",
    "            loss.backward()\n",
    "            for param in global_model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.data += lr * param.grad.data  # 梯度上升\n",
    "            optimizer.step()\n",
    "        print(\"loss:\",loss)\n",
    "    \n",
    "    print(\"Fine-tuning on remaining data...\")\n",
    "    global_model = federated_train(\n",
    "        global_model,\n",
    "        client_loaders,\n",
    "        criterion,\n",
    "        num_rounds=5,\n",
    "        num_local_epochs=1,\n",
    "        lr=0.001\n",
    "    )\n",
    "    \n",
    "    return global_model\n",
    "\n",
    "client_batch_size = 128\n",
    "\n",
    "# print(\"联邦训练完整模型...\")\n",
    "# # 初始化全局模型\n",
    "# full_net = LeNet(num_classes=10)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# global_round = 20\n",
    "\n",
    "# full_net = federated_train(\n",
    "#     full_net,\n",
    "#     client_loaders,  # 包含调整后的客户端3数据\n",
    "#     criterion,\n",
    "#     num_rounds=global_round,\n",
    "#     num_local_epochs=10,\n",
    "#     lr=0.001\n",
    "# )\n",
    "\n",
    "# # 未学习模型训练（从原始客户端加载器重建）\n",
    "# # 需要重新加载原始客户端数据（排除遗忘样本）\n",
    "# modified_client_loaders = [\n",
    "#     DataLoader(\n",
    "#         ds if idx != FORGOTTEN_CLIENT_IDX else Subset(ds.dataset, remaining_indices),\n",
    "#         batch_size=client_batch_size,\n",
    "#         shuffle=True\n",
    "#     )\n",
    "#     for idx, ds in enumerate(client_datasets)\n",
    "# ]\n",
    "\n",
    "# torch.save(full_net.state_dict(), \"federated_full_sample_1000_round_20_partial.pth\")\n",
    "full_net = LeNet(num_classes=10)\n",
    "full_model_path = \"/home/ecs-user/fgi/federated_weight/federated_full_sample_1000_round_20_partial.pth\"\n",
    "print(f\"Found existing full model at '{full_model_path}', loading weights...\")\n",
    "full_net.load_state_dict(torch.load(full_model_path))\n",
    "\n",
    "unlearned_net_ga = LeNet(num_classes=10)\n",
    "unlearned_net_ga.load_state_dict(full_net.state_dict())  # 从完整模型开始\n",
    "unlearned_net_ga = federated_unlearning_gradient_ascent(\n",
    "    unlearned_net_ga,\n",
    "    forgotten_loader,\n",
    "    client_loaders,\n",
    "    criterion,\n",
    "    num_rounds=150,\n",
    "    lr=0.001\n",
    ")\n",
    "torch.save(unlearned_net_ga.state_dict(), \"efficient_federated_unlearned_gradient_sample_1000_round_20.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-iid(取前五个类别进行训练)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import label_to_onehot, cross_entropy_for_onehot\n",
    "from models.vision import weights_init, LeNet\n",
    "num_classes=5\n",
    "\n",
    "net = LeNet(num_classes).to(\"cuda\")\n",
    "compress_rate = 1.0\n",
    "torch.manual_seed(1234)\n",
    "net.apply(weights_init)\n",
    "criterion = cross_entropy_for_onehot\n",
    "\n",
    "\n",
    "def federated_train(global_model, client_loaders, criterion, num_rounds=10, num_local_epochs=1, lr=0.001):\n",
    "    \"\"\"联邦训练函数(FedAvg)\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    global_model.to(device)\n",
    "    \n",
    "    for round in range(num_rounds):\n",
    "        print(f\"Communication Round {round+1}/{num_rounds}\")\n",
    "        client_models = []\n",
    "        \n",
    "        # 训练所有客户端\n",
    "        for client_id, loader in enumerate(client_loaders):\n",
    "            # 克隆全局模型\n",
    "            local_model = LeNet(num_classes=5)\n",
    "            local_model.load_state_dict(global_model.state_dict())\n",
    "            local_model.to(device)\n",
    "            optimizer = optim.Adam(local_model.parameters(), lr=lr)\n",
    "            \n",
    "            # 本地训练\n",
    "            local_model.train()\n",
    "            for _ in range(num_local_epochs):\n",
    "                for images, labels in loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = local_model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # 保存客户端模型参数\n",
    "            client_models.append(local_model.state_dict())\n",
    "        \n",
    "        # 参数平均（FedAvg）\n",
    "        global_dict = global_model.state_dict()\n",
    "        for key in global_dict.keys():\n",
    "            global_dict[key] = torch.stack(\n",
    "                [client_models[i][key].float() for i in range(len(client_models))], 0\n",
    "            ).mean(0)\n",
    "        global_model.load_state_dict(global_dict)\n",
    "    \n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证通过：遗忘数据集样本保持固定\n",
      "联邦训练完整模型...\n",
      "Communication Round 1/2\n",
      "Communication Round 2/2\n",
      "federated unlearning training...\n",
      "Communication Round 1/2\n",
      "Communication Round 2/2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from collections import defaultdict\n",
    "from utils import label_to_onehot, cross_entropy_for_onehot\n",
    "from models.vision import weights_init, LeNet\n",
    "num_classes=5\n",
    "\n",
    "net = LeNet(num_classes).to(\"cuda\")\n",
    "compress_rate = 1.0\n",
    "torch.manual_seed(1234)\n",
    "net.apply(weights_init)\n",
    "criterion = cross_entropy_for_onehot\n",
    "\n",
    "# 设置全局随机种子保证可重复性\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# 数据加载配置\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dst_train = datasets.CIFAR10(\n",
    "    root=\"~/.torch\", \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 选择前5个类别\n",
    "selected_classes = list(range(5))\n",
    "\n",
    "# 筛选出前5个类别的数据\n",
    "class_indices = defaultdict(list)\n",
    "for idx, (_, label) in enumerate(dst_train):\n",
    "    if label in selected_classes:\n",
    "        class_indices[label].append(idx)\n",
    "\n",
    "# 合并前5个类别的所有数据索引\n",
    "selected_indices = []\n",
    "for label in selected_classes:\n",
    "    selected_indices.extend(class_indices[label])\n",
    "\n",
    "# 使用筛选后的数据索引创建新的数据集\n",
    "filtered_dataset = Subset(dst_train, selected_indices)\n",
    "\n",
    "# 定义客户端数量和遗忘参数\n",
    "CLIENT_NUM = 4\n",
    "FORGOTTEN_CLIENT_IDX = 3  # 要遗忘的客户端索引\n",
    "FORGET_SIZE = 100        # 固定遗忘样本数\n",
    "\n",
    "# 固定划分客户端数据（使用确定性的随机划分）\n",
    "client_datasets = torch.utils.data.random_split(\n",
    "    filtered_dataset,\n",
    "    [len(filtered_dataset)//CLIENT_NUM]*CLIENT_NUM,\n",
    "    generator=torch.Generator().manual_seed(SEED)  # 固定划分随机种子\n",
    ")\n",
    "\n",
    "# 获取目标客户端原始数据索引\n",
    "target_dataset = client_datasets[FORGOTTEN_CLIENT_IDX]\n",
    "original_indices = target_dataset.indices.copy()  # 原始索引列表\n",
    "\n",
    "# 确定性地选择前N个样本作为遗忘集（方法1：绝对位置）\n",
    "fixed_forgotten_indices = sorted(original_indices)[:FORGET_SIZE]  # 按原始顺序取前256\n",
    "\n",
    "# 更新客户端数据集划分\n",
    "remaining_indices = list(set(original_indices) - set(fixed_forgotten_indices))\n",
    "client_datasets[FORGOTTEN_CLIENT_IDX] = Subset(filtered_dataset, remaining_indices)\n",
    "\n",
    "# 创建遗忘数据集加载器\n",
    "forgotten_dataset = Subset(filtered_dataset, fixed_forgotten_indices)\n",
    "forgotten_loader = DataLoader(\n",
    "    forgotten_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 创建客户端加载器（包含更新后的数据集）\n",
    "client_loaders = [\n",
    "    DataLoader(\n",
    "        ds, \n",
    "        batch_size=32, \n",
    "        shuffle=True,  # 训练时保持shuffle但随机种子固定\n",
    "        generator=torch.Generator().manual_seed(SEED))\n",
    "    for ds in client_datasets\n",
    "]\n",
    "\n",
    "# 验证固定遗忘样本\n",
    "def verify_fixed_samples():\n",
    "    # 第一次运行获取样本特征\n",
    "    first_run_samples = []\n",
    "    for batch in forgotten_loader:\n",
    "        first_run_samples.append(batch[0].sum().item())\n",
    "    first_sum = sum(first_run_samples)\n",
    "    \n",
    "    # 第二次运行应该完全相同\n",
    "    second_run_samples = []\n",
    "    for batch in forgotten_loader:\n",
    "        second_run_samples.append(batch[0].sum().item())\n",
    "    \n",
    "    assert np.allclose(first_sum, sum(second_run_samples)), \"样本不固定!\"\n",
    "    print(\"验证通过：遗忘数据集样本保持固定\")\n",
    "\n",
    "verify_fixed_samples()\n",
    "\n",
    "# 选择参与训练的类别（前5个类别）\n",
    "def filter_classes_for_training(data, selected_classes):\n",
    "    return [item for item in data if item[1] in selected_classes]\n",
    "\n",
    "# Filter data for training (learning and unlearning)\n",
    "filtered_train_data = filter_classes_for_training(dst_train, selected_classes)\n",
    "\n",
    "# 更新客户端加载器（仅包含前5个类别的数据）\n",
    "modified_client_loaders = [\n",
    "    DataLoader(\n",
    "        Subset(filtered_train_data, ds.indices),\n",
    "        batch_size=128,\n",
    "        shuffle=True\n",
    "    )\n",
    "    for idx, ds in enumerate(client_datasets)\n",
    "]\n",
    "\n",
    "# 联邦训练\n",
    "print(\"联邦训练完整模型...\")\n",
    "full_net = LeNet(num_classes=5)  # 更新类别数为5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "global_round = 2\n",
    "\n",
    "full_net = federated_train(\n",
    "    full_net,\n",
    "    modified_client_loaders,  # 包含前5个类别数据的客户端加载器\n",
    "    criterion,\n",
    "    num_rounds=global_round,\n",
    "    num_local_epochs=1,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# 未学习模型训练（从原始客户端加载器重建）\n",
    "# 需要重新加载原始客户端数据（排除遗忘样本）\n",
    "modified_client_loaders_unlearning = [\n",
    "    DataLoader(\n",
    "        ds if idx != FORGOTTEN_CLIENT_IDX else Subset(ds.dataset, remaining_indices),\n",
    "        batch_size=128,\n",
    "        shuffle=True\n",
    "    )\n",
    "    for idx, ds in enumerate(client_datasets)\n",
    "]\n",
    "\n",
    "unlearned_net = LeNet(num_classes=5)\n",
    "print(\"federated unlearning training...\")\n",
    "unlearned_net = federated_train(\n",
    "    unlearned_net,\n",
    "    modified_client_loaders_unlearning,  # 使用排除遗忘样本的加载器\n",
    "    criterion,\n",
    "    num_rounds=global_round,\n",
    "    num_local_epochs=1,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(full_net.state_dict(), \"/home/ecs-user/fgi/federated_weight/noniid_federated_full_sample_100_round_2_partial.pth\")\n",
    "torch.save(unlearned_net.state_dict(), \"/home/ecs-user/fgi/federated_weight/noniid_federated_unlearned_sample_100_round_2_partial.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_classes = list(range(5, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dst_train = datasets.CIFAR10(\"~/.torch\", train=True, download=True, transform=transform)\n",
    "dst_test = datasets.CIFAR10(\"~/.torch\", train=False, download=True, transform=transform)\n",
    "\n",
    "# 划分训练数据到4个客户端（IID划分）\n",
    "client_num = 4\n",
    "client_datasets = torch.utils.data.random_split(\n",
    "    dst_train, \n",
    "    [len(dst_train)//client_num]*client_num\n",
    ")\n",
    "\n",
    "# 创建客户端DataLoader\n",
    "client_batch_size = 32\n",
    "client_loaders = [\n",
    "    DataLoader(ds, batch_size=client_batch_size, shuffle=True) \n",
    "    for ds in client_datasets\n",
    "]\n",
    "\n",
    "# 初始化全局模型\n",
    "full_net = LeNet(num_classes=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 联邦训练（完整数据）\n",
    "print(\"联邦训练全局模型...\")\n",
    "full_net = federated_train(\n",
    "    full_net,\n",
    "    client_loaders,\n",
    "    criterion,\n",
    "    num_rounds=50,      # 通信轮次\n",
    "    num_local_epochs=1, # 每个客户端本地训练epoch数\n",
    "    lr=0.001\n",
    ")\n",
    "torch.save(full_net.state_dict(), \"federated_full_model.pth\")\n",
    "\n",
    "# 联邦遗忘（排除第3个客户端）\n",
    "forgotten_client_idx = 3\n",
    "remaining_loaders = [loader for idx, loader in enumerate(client_loaders) if idx != forgotten_client_idx]\n",
    "\n",
    "# 初始化新全局模型\n",
    "unlearned_net = LeNet(num_classes=10)\n",
    "\n",
    "print(\"联邦未学习训练...\")\n",
    "unlearned_net = federated_train(\n",
    "    unlearned_net,\n",
    "    remaining_loaders,\n",
    "    criterion,\n",
    "    num_rounds=50,\n",
    "    num_local_epochs=1,\n",
    "    lr=0.001\n",
    ")\n",
    "torch.save(unlearned_net.state_dict(), \"federated_unlearned_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import label_to_onehot, cross_entropy_for_onehot\n",
    "from models.vision import weights_init, LeNet\n",
    "from models.resnet import resnet20\n",
    "num_classes=10\n",
    "\n",
    "net = resnet20(num_classes).to(\"cuda\")\n",
    "compress_rate = 1.0\n",
    "torch.manual_seed(1234)\n",
    "net.apply(weights_init)\n",
    "criterion = cross_entropy_for_onehot\n",
    "\n",
    "\n",
    "def federated_train(global_model, client_loaders, criterion, num_rounds=10, num_local_epochs=1, lr=0.001):\n",
    "    \"\"\"联邦训练函数(FedAvg)\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    global_model.to(device)\n",
    "    \n",
    "    for round in range(num_rounds):\n",
    "        print(f\"Communication Round {round+1}/{num_rounds}\")\n",
    "        client_models = []\n",
    "        \n",
    "        # 训练所有客户端\n",
    "        for client_id, loader in enumerate(client_loaders):\n",
    "            # 克隆全局模型\n",
    "            local_model = resnet20(num_classes=10)\n",
    "            local_model.load_state_dict(global_model.state_dict())\n",
    "            local_model.to(device)\n",
    "            optimizer = optim.Adam(local_model.parameters(), lr=lr)\n",
    "            \n",
    "            # 本地训练\n",
    "            local_model.train()\n",
    "            for _ in range(num_local_epochs):\n",
    "                for images, labels in loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = local_model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # 保存客户端模型参数\n",
    "            client_models.append(local_model.state_dict())\n",
    "        \n",
    "        # 参数平均（FedAvg）\n",
    "        global_dict = global_model.state_dict()\n",
    "        for key in global_dict.keys():\n",
    "            global_dict[key] = torch.stack(\n",
    "                [client_models[i][key].float() for i in range(len(client_models))], 0\n",
    "            ).mean(0)\n",
    "        global_model.load_state_dict(global_dict)\n",
    "    \n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "验证通过：遗忘数据集样本保持固定\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# 设置全局随机种子保证可重复性\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# 数据加载配置\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dst_train = datasets.CIFAR10(\n",
    "    root=\"~/.torch\", \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# 定义客户端数量和遗忘参数\n",
    "CLIENT_NUM = 4\n",
    "FORGOTTEN_CLIENT_IDX = 3  # 要遗忘的客户端索引\n",
    "FORGET_SIZE = 256        # 固定遗忘样本数\n",
    "\n",
    "\n",
    "# 固定划分客户端数据（使用确定性的随机划分）\n",
    "client_datasets = torch.utils.data.random_split(\n",
    "    dst_train,\n",
    "    [len(dst_train)//CLIENT_NUM]*CLIENT_NUM,\n",
    "    generator=torch.Generator().manual_seed(SEED)  # 固定划分随机种子\n",
    ")\n",
    "\n",
    "# 获取目标客户端原始数据索引\n",
    "target_dataset = client_datasets[FORGOTTEN_CLIENT_IDX]\n",
    "original_indices = target_dataset.indices.copy()  # 原始索引列表\n",
    "\n",
    "# 确定性地选择前N个样本作为遗忘集（方法1：绝对位置）\n",
    "fixed_forgotten_indices = sorted(original_indices)[:FORGET_SIZE]  # 按原始顺序取前1000\n",
    "\n",
    "# 更新客户端数据集划分\n",
    "remaining_indices = list(set(original_indices) - set(fixed_forgotten_indices))\n",
    "client_datasets[FORGOTTEN_CLIENT_IDX] = Subset(dst_train, remaining_indices)\n",
    "\n",
    "# 创建遗忘数据集加载器\n",
    "forgotten_dataset = Subset(dst_train, fixed_forgotten_indices)\n",
    "forgotten_loader = DataLoader(\n",
    "    forgotten_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 创建客户端加载器（包含更新后的数据集）\n",
    "client_loaders = [\n",
    "    DataLoader(\n",
    "        ds, \n",
    "        batch_size=32, \n",
    "        shuffle=True,  # 训练时保持shuffle但随机种子固定\n",
    "        generator=torch.Generator().manual_seed(SEED))\n",
    "    for ds in client_datasets ]\n",
    "\n",
    "# 验证固定遗忘样本\n",
    "def verify_fixed_samples():\n",
    "    # 第一次运行获取样本特征\n",
    "    first_run_samples = []\n",
    "    for batch in forgotten_loader:\n",
    "        first_run_samples.append(batch[0].sum().item())\n",
    "    first_sum = sum(first_run_samples)\n",
    "    \n",
    "    # 第二次运行应该完全相同\n",
    "    second_run_samples = []\n",
    "    for batch in forgotten_loader:\n",
    "        second_run_samples.append(batch[0].sum().item())\n",
    "    \n",
    "    assert np.allclose(first_sum, sum(second_run_samples)), \"样本不固定!\"\n",
    "    print(\"验证通过：遗忘数据集样本保持固定\")\n",
    "\n",
    "verify_fixed_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "联邦训练完整模型...\n",
      "Communication Round 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communication Round 2/50\n",
      "Communication Round 3/50\n",
      "Communication Round 4/50\n",
      "Communication Round 5/50\n",
      "Communication Round 6/50\n",
      "Communication Round 7/50\n",
      "Communication Round 8/50\n",
      "Communication Round 9/50\n",
      "Communication Round 10/50\n",
      "Communication Round 11/50\n",
      "Communication Round 12/50\n",
      "Communication Round 13/50\n",
      "Communication Round 14/50\n",
      "Communication Round 15/50\n",
      "Communication Round 16/50\n",
      "Communication Round 17/50\n",
      "Communication Round 18/50\n",
      "Communication Round 19/50\n",
      "Communication Round 20/50\n",
      "Communication Round 21/50\n",
      "Communication Round 22/50\n",
      "Communication Round 23/50\n",
      "Communication Round 24/50\n",
      "Communication Round 25/50\n",
      "Communication Round 26/50\n",
      "Communication Round 27/50\n",
      "Communication Round 28/50\n",
      "Communication Round 29/50\n",
      "Communication Round 30/50\n",
      "Communication Round 31/50\n",
      "Communication Round 32/50\n",
      "Communication Round 33/50\n",
      "Communication Round 34/50\n",
      "Communication Round 35/50\n",
      "Communication Round 36/50\n",
      "Communication Round 37/50\n",
      "Communication Round 38/50\n",
      "Communication Round 39/50\n",
      "Communication Round 40/50\n",
      "Communication Round 41/50\n",
      "Communication Round 42/50\n",
      "Communication Round 43/50\n",
      "Communication Round 44/50\n",
      "Communication Round 45/50\n",
      "Communication Round 46/50\n",
      "Communication Round 47/50\n",
      "Communication Round 48/50\n",
      "Communication Round 49/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     12\u001b[0m global_round \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 14\u001b[0m full_net \u001b[38;5;241m=\u001b[39m \u001b[43mfederated_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_net\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 包含调整后的客户端3数据\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_local_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 未学习模型训练（从原始客户端加载器重建）\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 需要重新加载原始客户端数据（排除遗忘样本）\u001b[39;00m\n\u001b[1;32m     25\u001b[0m modified_client_loaders \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     DataLoader(\n\u001b[1;32m     27\u001b[0m         ds \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m!=\u001b[39m FORGOTTEN_CLIENT_IDX \u001b[38;5;28;01melse\u001b[39;00m Subset(ds\u001b[38;5;241m.\u001b[39mdataset, remaining_indices),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, ds \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(client_datasets)\n\u001b[1;32m     32\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m, in \u001b[0;36mfederated_train\u001b[0;34m(global_model, client_loaders, criterion, num_rounds, num_local_epochs, lr)\u001b[0m\n\u001b[1;32m     41\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 43\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     45\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Learning_to_Invert/models/resnet.py:113\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(out)\n\u001b[1;32m    112\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(out)\n\u001b[0;32m--> 113\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mavg_pool2d(out, out\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    115\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Learning_to_Invert/models/resnet.py:82\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     80\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out))\n\u001b[1;32m     81\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortcut(x)\n\u001b[0;32m---> 82\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "client_batch_size = 128\n",
    "\n",
    "print(\"联邦训练完整模型...\")\n",
    "# 初始化全局模型\n",
    "full_net = resnet20(num_classes).to(\"cuda\")\n",
    "compress_rate = 0.5\n",
    "torch.manual_seed(1234)\n",
    "full_net.apply(weights_init)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "global_round = 50\n",
    "\n",
    "full_net = federated_train(\n",
    "    full_net,\n",
    "    client_loaders,  # 包含调整后的客户端3数据\n",
    "    criterion,\n",
    "    num_rounds=global_round,\n",
    "    num_local_epochs=1,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# 未学习模型训练（从原始客户端加载器重建）\n",
    "# 需要重新加载原始客户端数据（排除遗忘样本）\n",
    "modified_client_loaders = [\n",
    "    DataLoader(\n",
    "        ds if idx != FORGOTTEN_CLIENT_IDX else Subset(ds.dataset, remaining_indices),\n",
    "        batch_size=client_batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    for idx, ds in enumerate(client_datasets)\n",
    "]\n",
    "\n",
    "unlearned_net = resnet20(num_classes=10)\n",
    "print(\"federated unlearning training...\")\n",
    "unlearned_net = federated_train(\n",
    "    unlearned_net,\n",
    "    modified_client_loaders,  # 使用排除遗忘样本的加载器\n",
    "    criterion,\n",
    "    num_rounds=global_round,\n",
    "    num_local_epochs=1,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(full_net.state_dict(), \"federated_full_resnet_sample_256_round_50_partial.pth\")\n",
    "torch.save(unlearned_net.state_dict(), \"federated_unlearned_resnet_sample_256_round_50_partial.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
